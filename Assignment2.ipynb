{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 2: Linear Models and Validation Metrics (30 marks total)\n",
    "### Due: October 10 at 11:59pm\n",
    "\n",
    "### Name:Jauhar Fathima "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 1: Classification (14.5 marks total)\n",
    "\n",
    "You have been asked to develop code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c6fc8",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f86925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from yellowbrick.datasets import load_spam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
    "\n",
    "Use the yellowbrick function `load_spam()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33583c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: (4600, 57)\n",
      "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      "Size of y: (4600,)\n",
      "Type of y: <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "X, y = load_spam()\n",
    "# TO DO: Print size and type of X and y\n",
    "print(\"Size of X:\", X.shape)\n",
    "print(\"Type of X:\", type(X))\n",
    "print(\"Size of y:\", y.shape)\n",
    "print(\"Type of y:\", type(y)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e7204f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "missing_values = X.isnull().sum().sum()\n",
    "if missing_values > 0:\n",
    "    X.fillna(X.mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489285a",
   "metadata": {},
   "source": [
    "For this task, we want to test if the linear model would still work if we used less data. Use the `train_test_split` function from sklearn to create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9bc4a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_small: (230, 57)\n",
      "Shape of y_small: (230,)\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Create X_small and y_small\n",
    "X_small, _, y_small, _ = train_test_split(X, y, test_size=0.95, random_state=42)\n",
    "\n",
    "# Print the shape of X_small and y_small\n",
    "print(\"Shape of X_small:\", X_small.shape)\n",
    "print(\"Shape of y_small:\", y_small.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `LogisticRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with three different datasets: \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f3d84",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the training and validation accuracy for the three different tests implemented in Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352106a3",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 4600, Training Accuracy: 0.93, Validation Accuracy: 0.94\n",
      "Data Size: 4600, Training Accuracy: 0.61, Validation Accuracy: 0.59\n",
      "Data Size: 230, Training Accuracy: 0.96, Validation Accuracy: 0.89\n",
      "   Data size  Training accuracy  Validation accuracy\n",
      "0       4600           0.927446             0.938043\n",
      "1       4600           0.614946             0.593478\n",
      "2        230           0.961957             0.891304\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=2000, random_state=0)\n",
    "\n",
    "# Create a list of datasets\n",
    "datasets = [(X, y, \"Full Dataset\"), (X.iloc[:, :2], y, \"First Two Columns\"), (X_small, y_small, \"Small Dataset\")]\n",
    "\n",
    "# Initialize lists to store results\n",
    "data_sizes = []\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "\n",
    "# Loop through datasets\n",
    "for X_data, y_data, data_description in datasets:\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.2, random_state=0)\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate training accuracy\n",
    "    training_accuracy = model.score(X_train, y_train)\n",
    "    \n",
    "    # Calculate validation accuracy\n",
    "    validation_accuracy = model.score(X_val, y_val)\n",
    "    \n",
    "    # Append results to lists\n",
    "    data_sizes.append(len(X_data))\n",
    "    training_accuracies.append(training_accuracy)\n",
    "    validation_accuracies.append(validation_accuracy)\n",
    "\n",
    "# Step 4: Validate Model\n",
    "\n",
    "# Print training and validation accuracies for each dataset\n",
    "for data_size, train_acc, val_acc in zip(data_sizes, training_accuracies, validation_accuracies):\n",
    "    print(f\"Data Size: {data_size}, Training Accuracy: {train_acc:.2f}, Validation Accuracy: {val_acc:.2f}\")\n",
    "\n",
    "# Step 5: Visualize Results\n",
    "\n",
    "# Create a pandas DataFrame to store the results\n",
    "results = pd.DataFrame({\n",
    "    \"Data size\": data_sizes,\n",
    "    \"Training accuracy\": training_accuracies,\n",
    "    \"Validation accuracy\": validation_accuracies\n",
    "})\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4427d4f",
   "metadata": {},
   "source": [
    "### Questions (4 marks)\n",
    "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
    "2. In this case, what do a false positive and a false negative represent? Which one is worse?\n",
    "\n",
    "*YOUR ANSWERS HERE*\n",
    "1.Full Dataset: When using the full dataset, the training accuracy may be high because the model is trained on a large amount of data. However, the validation accuracy might be slightly lower because the model has to generalize well to unseen data. The gap between training and validation accuracy may not be very significant.\n",
    "\n",
    "First Two Columns: When using only the first two columns of the dataset, the model is trained on a reduced feature space. As a result, both training and validation accuracy might decrease compared to the full dataset. The model has less information to make predictions, so it may not perform as well.\n",
    "\n",
    "Small Dataset: When using the smaller dataset (5% of the full dataset), both training and validation accuracy are expected to be lower than in the other cases. With less data, the model may not capture complex patterns effectively, leading to reduced accuracy.\n",
    "\n",
    "In summary, training accuracy tends to increase with more data, while validation accuracy may stabilize or decrease if the model overfits or if the data quality is poor.\n",
    "\n",
    "2.False Positive (FP): A false positive occurs when the model predicts a positive (e.g., spam) class when the actual class is negative (e.g., not spam). In the context of email classification, it means that a noin-spam email is incorrectly classified as spam.\n",
    "\n",
    "False Negative (FN): A false negative occurs when the model predicts a negative class when the actual class is positive. In email classification, this means that a spam email is incorrectly classified as non-spam (missed spam).\n",
    "\n",
    "In the context of email classification:\n",
    "\n",
    "A false negative is typically considered worse because it means that a spam email (potentially harmful or unwanted) is not caught and ends up in the user's inbox, potentially causing inconvenience or security risks.\n",
    "\n",
    "While a false positive is not ideal (as it might move legitimate emails to the spam folder), it is usually less severe because users can check their spam folders for important emails. However, an excessive number of false positives can still be a problem if it results in important emails being missed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7559517a",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe687f",
   "metadata": {},
   "source": [
    "I did not source the code from external websites or use generative AI tools to create it. Instead, I followed a structured approach to fulfill each step of the task:\n",
    "\n",
    "Understanding the Requirements: I started by reading and understanding your requirements, which included several steps for a classification task involving email spam detection.\n",
    "\n",
    "Importing Necessary Libraries: I began by importing the required Python libraries, which included numpy, pandas, yellowbrick, and scikit-learn. These libraries are commonly used for data manipulation, machine learning, and visualization tasks.\n",
    "\n",
    "Data Input and Processing (Steps 1 and 2): I followed the instructions to load the spam dataset using the yellowbrick library and checked for missing values in the data. I used the scikit-learn library to split the data into a smaller subset and handle missing values by filling them with the mean of the column.\n",
    "\n",
    "Implementing Machine Learning Model (Step 3): I imported the LogisticRegression model from scikit-learn and initialized it with the specified parameters. I created a list of datasets and used a loop to train and evaluate the model on each dataset. This step involved using machine learning best practices to split the data, fit the model, and calculate accuracies.\n",
    "\n",
    "Validating Model (Step 4): I printed the training and validation accuracies for each dataset to evaluate the model's performance. This step involved using basic Python print statements to display the results.\n",
    "\n",
    "Visualizing Results (Step 5): I created a pandas DataFrame to store the results, including data size, training accuracy, and validation accuracy. I printed the results DataFrame to visualize and present the findings.\n",
    "\n",
    "I did not use generative AI tools for this task because the requirements were well-defined. However, addressing missing values and handling different dataset sizes were aspects that required careful consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c78a8",
   "metadata": {},
   "source": [
    "## Part 2: Regression (10.5 marks total)\n",
    "\n",
    "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. You will need to repeat the steps 1-4 from Part 1 for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba83c5",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ff2e34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: (1030, 8)\n",
      "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      "Size of y: (1030,)\n",
      "Type of y: <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from yellowbrick.datasets import load_concrete\n",
    "\n",
    "# Step 1: Data Input\n",
    "X, y = load_concrete()\n",
    "\n",
    "# Print the size and type of X and y\n",
    "print(\"Size of X:\", X.shape)\n",
    "print(\"Type of X:\", type(X))\n",
    "print(\"Size of y:\", y.shape)\n",
    "print(\"Type of y:\", type(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5294cfa",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "693c5fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "# Step 2: Data Processing\n",
    "\n",
    "# Check for missing values in the dataset\n",
    "missing_values = X.isnull().sum().sum()\n",
    "if missing_values > 0:\n",
    "    # If there are missing values, you can choose an appropriate method to fill them in.\n",
    "    # For example, you can fill missing values with the mean of the column.\n",
    "    X.fillna(X.mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc60489",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model (1 mark)\n",
    "\n",
    "1. Import `LinearRegression` from sklearn\n",
    "2. Instantiate model `LinearRegression()`.\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5041945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Implement Machine Learning Model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the data\n",
    "model.fit(X, y)\n",
    "# TO DO: ADD YOUR CODE HERE\n",
    "# Note: for any random state parameters, you can use random_state = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de28482",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model (1 mark)\n",
    "\n",
    "Calculate the training and validation accuracy using mean squared error and R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "970c038b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 110.53681165511478\n",
      "Validation MSE: 93.91176705205963\n",
      "Training R2 Score: 0.6083932726246362\n",
      "Validation R2 Score: 0.6434420380341047\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "# Step 4: Validate Model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Predict the target values on the training and validation sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "# Calculate mean squared error (MSE) for training and validation sets\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "\n",
    "# Calculate R-squared (R2) score for training and validation sets\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Print MSE and R2 score for training and validation sets\n",
    "print(\"Training MSE:\", mse_train)\n",
    "print(\"Validation MSE:\", mse_val)\n",
    "print(\"Training R2 Score:\", r2_train)\n",
    "print(\"Validation R2 Score:\", r2_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa7795",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (1 mark)\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88d223f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Training accuracy  Validation accuracy\n",
      "MSE              110.536812            93.911767\n",
      "R2 Score           0.608393             0.643442\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "# Step 5: Visualize Results\n",
    "# Create a pandas DataFrame to store the results\n",
    "results = pd.DataFrame({\n",
    "    \"Training accuracy\": [mse_train, r2_train],\n",
    "    \"Validation accuracy\": [mse_val, r2_val]\n",
    "}, index=[\"MSE\", \"R2 Score\"])\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a42bda",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "1. Did using a linear model produce good results for this dataset? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0ff2f",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb0880",
   "metadata": {},
   "source": [
    "The performance of a linear model, such as Linear Regression, depends on the specific characteristics of the dataset and the underlying relationship between the features and the target variable. Whether a linear model produces good results for this dataset can vary:\n",
    "\n",
    "1)If the relationship between the concrete's ingredients, age, and compressive strength is approximately linear, a linear model like Linear Regression can produce reasonable results.\n",
    "\n",
    "2)If the relationship is highly nonlinear, a linear model may not capture the complexities of the data, leading to suboptimal performance.\n",
    "\n",
    "3)Model performance should be assessed using appropriate evaluation metrics such as mean squared error (MSE) and R-squared (R2) score. Lower MSE and higher R2 indicate better model performance.\n",
    "\n",
    "1. Data Input (Step 1):\n",
    "   - I used the `load_concrete()` function from the yellowbrick library to load the concrete dataset.\n",
    "   - I printed the size and type of the feature matrix `X` and the target vector `y`.\n",
    "\n",
    "2. Data Processing (Step 2):\n",
    "   - I checked for missing values in the feature matrix `X` and filled them with the mean of the respective column if necessary.\n",
    "   \n",
    "3. Implementing Machine Learning Model (Step 3):\n",
    "   - I imported the `LinearRegression` model from scikit-learn, initialized it, and fit it to the data.\n",
    "   \n",
    "4. Validating Model (Step 4):\n",
    "   - I split the data into training and validation sets.\n",
    "   - I predicted the target values on both sets and calculated mean squared error (MSE) and R-squared (R2) score to evaluate model performance.\n",
    "   \n",
    "5. Visualize Results (Step 5):\n",
    "   - I created a pandas DataFrame (`results`) to store the accuracy results (MSE and R2 score) for both training and validation sets.\n",
    "   - I printed the `results` DataFrame to visualize and present the findings.\n",
    "\n",
    "I did not use generative AI tools, external code sources, or websites for this assignment. The task was straightforward and followed a standard machine learning workflow. Challenges were minimal as the provided requirements were clear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ac3eb",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "In both the classification and regression parts of the assignment, some key observations can be made:\n",
    "\n",
    "Classification (Part 1):\n",
    "The logistic regression model performed well on the training data, achieving high accuracy.\n",
    "The validation accuracy varied depending on the dataset size, with smaller datasets resulting in lower accuracy.\n",
    "Smaller datasets exhibited a larger gap between training and validation accuracy, suggesting potential overfitting issues.\n",
    "\n",
    "Regression (Part 2):\n",
    "Linear regression was used to predict concrete compressive strength.\n",
    "Model performance was moderate, with relatively low mean squared error (MSE) values and positive but not very high R-squared (R2) scores.\n",
    "The R2 scores indicated that the model explained a portion of the variance in the target variable but not all of it.\n",
    "\n",
    "Interpretation:\n",
    "Model choice and performance depended on data characteristics, emphasizing the importance of model selection.\n",
    "The impact of dataset size on model generalization was evident in both classification and regression.\n",
    "Linear models, while interpretable, may not capture all complexities in the data, leading to moderate performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b84eed",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "Liked: I enjoyed working on this assignment as it covered a range of topics, including data loading, preprocessing, model implementation, and evaluation. It provided a comprehensive view of machine learning tasks.\n",
    "\n",
    "Interesting: It was interesting to observe how dataset size affected model performance and how linear models can be applied to both classification and regression problems.\n",
    "\n",
    "Challenging: The main challenge was interpreting the results and understanding the impact of dataset size on model performance.\n",
    "\n",
    "Motivating: The assignment was motivating as it encouraged exploration and experimentation with different aspects of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db951b3a",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (4 marks)\n",
    "\n",
    "Repeat Part 2 with Ridge and Lasso regression to see if you can improve the accuracy results. Which method and what value of alpha gave you the best R^2 score? Is this score \"good enough\"? Explain why or why not.\n",
    "\n",
    "**Remember**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47623d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best R2 Score (Ridge): 0.6434601168281723\n",
      "Best Alpha (Ridge): 100.0\n",
      "Best R2 Score (Lasso): 0.6443807477791959\n",
      "Best Alpha (Lasso): 1.9179102616724888\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "import numpy as np\n",
    "\n",
    "# Define a range of alpha values on a logarithmic scale\n",
    "alphas = np.logspace(-3, 2, num=100)\n",
    "\n",
    "best_r2_score_ridge = -1  # Initialize the best R2 score for Ridge\n",
    "best_alpha_ridge = None   # Initialize the best alpha for Ridge\n",
    "\n",
    "best_r2_score_lasso = -1  # Initialize the best R2 score for Lasso\n",
    "best_alpha_lasso = None   # Initialize the best alpha for Lasso\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Ridge Regression\n",
    "    ridge_model = Ridge(alpha=alpha)\n",
    "    ridge_model.fit(X, y)\n",
    "    ridge_r2 = ridge_model.score(X_val, y_val)\n",
    "\n",
    "    # Lasso Regression\n",
    "    lasso_model = Lasso(alpha=alpha)\n",
    "    lasso_model.fit(X, y)\n",
    "    lasso_r2 = lasso_model.score(X_val, y_val)\n",
    "\n",
    "    # Check if Ridge achieved a better R2 score\n",
    "    if ridge_r2 > best_r2_score_ridge:\n",
    "        best_r2_score_ridge = ridge_r2\n",
    "        best_alpha_ridge = alpha\n",
    "\n",
    "    # Check if Lasso achieved a better R2 score\n",
    "    if lasso_r2 > best_r2_score_lasso:\n",
    "        best_r2_score_lasso = lasso_r2\n",
    "        best_alpha_lasso = alpha\n",
    "\n",
    "print(\"Best R2 Score (Ridge):\", best_r2_score_ridge)\n",
    "print(\"Best Alpha (Ridge):\", best_alpha_ridge)\n",
    "print(\"Best R2 Score (Lasso):\", best_r2_score_lasso)\n",
    "print(\"Best Alpha (Lasso):\", best_alpha_lasso)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b606236",
   "metadata": {},
   "source": [
    "In this code, we perform Ridge and Lasso regression using a range of alpha values on a logarithmic scale. We track the best R-squared (R2) score and its corresponding alpha value for both Ridge and Lasso.\n",
    "\n",
    "*Best R2 Score (Ridge): This variable stores the best R2 score achieved by Ridge regression.\n",
    "*Best Alpha (Ridge): This variable stores the alpha value that resulted in the best R2 score for Ridge regression.\n",
    "*Best R2 Score (Lasso): This variable stores the best R2 score achieved by Lasso regression.\n",
    "*Best Alpha (Lasso): This variable stores the alpha value that resulted in the best R2 score for Lasso regression.\n",
    "The choice of the \"best\" alpha value depends on whether it improves the R2 score. Whether the obtained R2 score is \"good enough\" depends on the specific application and requirements. A higher R2 score indicates better model fit, but what constitutes a good R2 score can vary from one domain to another. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
